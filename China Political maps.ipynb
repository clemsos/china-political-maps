{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping political networks in China\n",
    "\n",
    "> This is an ongoing work.\n",
    "\n",
    "We attempt to draw a map of the unfolding of political networks in greater China. \n",
    "\n",
    "The present document relates the methodology we use to explore the data and answer our first questions.\n",
    "\n",
    "\n",
    "* What are the exchanges between provinces over time ?\n",
    "* Which provinces are exchanging the most people ?\n",
    "* What is the particular pattern emerging fomr specific cities in this process ?  \n",
    "\n",
    "We want to see how politicians are moving from one city to another, to interrogate how the experience developed in one specific place can be transfered to another over time. Therefore, we want to study how Chinese politicans travel and resettle along their carreers.\n",
    "\n",
    "We rely on biographic data of 4705 politicians extracted from the website [China Vitae](http://chinavitae.com).\n",
    "\n",
    "We analyse their carreer paths to create network maps of movement of politicians in China.\n",
    "We use Python (and the present notebook) for data analysis and the software [Topogram](http://topogram.io) to create network visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import logging \n",
    "import datetime\n",
    "\n",
    "from slugify import slugify\n",
    "\n",
    "import pymongo\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.readwrite import write_gpickle\n",
    "\n",
    "from topogram_client import TopogramAPIClient\n",
    "\n",
    "now=datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "# passwords\n",
    "TOPOGRAM_URL = \"https://app.topogram.io\" # http://localhost:3000\n",
    "USER = \"clement.renaud@epfl.ch\"\n",
    "PASSWORD = \"makerspaces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# connect to the topogram instance \n",
    "topogram = TopogramAPIClient(TOPOGRAM_URL)\n",
    "\n",
    "# topogram.create_user(USER, PASSWORD)\n",
    "topogram.user_login(USER, PASSWORD)\n",
    "\n",
    "\n",
    "def get_slug(name, type):\n",
    "    \"\"\" get a clean string ID from name and type\"\"\"\n",
    "    return \"%s-%s\"%(slugify( name.decode('utf-8') ),type.decode('utf-8'))\n",
    "\n",
    "\n",
    "def create_node(name, type, start, end, orga=None, info=None) : \n",
    "    \"\"\"create the node at the right format in the main graph\"\"\"\n",
    "    slug = get_slug(name, type)\n",
    "    \n",
    "    try :\n",
    "        if start > G.node[slug][\"start\"] : start =  G.node[slug][\"start\"]\n",
    "        if end > G.node[slug][\"end\"] : start =  G.node[slug][\"end\"]\n",
    "    except:\n",
    "        start = start\n",
    "        end = end\n",
    "            \n",
    "    node = {}\n",
    "    node[\"id\"] = slug\n",
    "    node[\"type\"] = type\n",
    "    node[\"orga\"] = orga # cluster or ARC ?\n",
    "    node[\"name\"] = name\n",
    "    node[\"start\"] = start\n",
    "    node[\"end\"] = end\n",
    "    \n",
    "    if info :\n",
    "        node[\"info\"]=info\n",
    "    \n",
    "    G.add_node(node[\"id\"], node)\n",
    "    return node[\"id\"]\n",
    "\n",
    "def merge_edge_data(Graph, e, data):\n",
    "    \"\"\"\n",
    "    merge data properly :prevent data within existing edges to be erased\n",
    "    \"\"\"\n",
    "    try : \n",
    "        Graph.edge[e[0]][e[1]]\n",
    "    except KeyError:\n",
    "        Graph.add_edge(e[0], e[1])\n",
    "        \n",
    "    try:\n",
    "        Graph.edge[e[0]][e[1]][\"edge_types\"].append(data)\n",
    "    except KeyError:\n",
    "        Graph.edge[e[0]][e[1]][\"edge_types\"] = [data]\n",
    "\n",
    "\n",
    "# functions for Topogram\n",
    "def parse_nodes(Graph):\n",
    "    # create the graph\n",
    "    nodes = []\n",
    "    for n in Graph.nodes(data=True): \n",
    "        if Graph.degree(n[0]) > 0: # ignore singletons\n",
    "            node = G.node[n[0]]\n",
    "            node[\"id\"] = n[0]\n",
    "            nodes.append(node)\n",
    "\n",
    "    print \"%s nodes\"%len(nodes)\n",
    "    return nodes\n",
    "\n",
    "def parse_edges(Graph):\n",
    "    edges = []\n",
    "    for i, e in enumerate(Graph.edges(data=True)): \n",
    "        edge = e[2] \n",
    "        edge[\"source\"] = e[0]\n",
    "        edge[\"target\"] = e[1]\n",
    "\n",
    "        edges.append(edge)\n",
    "\n",
    "    print \"%s edges\"%len(edges)\n",
    "    return edges\n",
    "    \n",
    "def create_topogram(title, nodes, edges): \n",
    "    \n",
    "    print \"Creating topogram '%s'\"%title\n",
    "    \n",
    "    r = topogram.create_topogram(title)\n",
    "    print r[\"message\"]\n",
    "    topogram_ID = r[\"data\"][0][\"_id\"]\n",
    "\n",
    "    # get and backup existing nodes and edges\n",
    "    existing_nodes = topogram.get_nodes(topogram_ID)[\"data\"]\n",
    "    existing_edges = topogram.get_edges(topogram_ID)[\"data\"]\n",
    "\n",
    "    # clear existing graph\n",
    "    topogram.delete_nodes([n[\"_id\"] for n in existing_nodes])\n",
    "    print \"nodes deleted\"\n",
    "    topogram.delete_edges([n[\"_id\"] for n in existing_edges])\n",
    "    print \"edges deleted\"\n",
    "\n",
    "    r = topogram.create_nodes(topogram_ID, nodes)\n",
    "    print \"%s nodes created.\"%len(r[\"data\"])\n",
    "    r = topogram.create_edges(topogram_ID, edges)\n",
    "    print \"%s edges created.\"%len(r[\"data\"])\n",
    "\n",
    "    print \"done. Topogram is online at %s/topograms/%s/view\"%(TOPOGRAM_URL, topogram_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Fetch data from the copy of database\n",
    "\n",
    "Previously extracted from China Vitae website on Feb, 21 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to mongo\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client.chinaVitae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network of locations\n",
    "\n",
    "\n",
    "During his/her carreer, each politican occupied a number of sucessive positions. These positions are located in specific regions and institutions. For each line in the CV, the original data contains two main components (location and institution). \n",
    "\n",
    "Here, we decided to extract all locations (nodes in the graph). \n",
    "Each time two locations are mentioned in a single person's carrer, we add a link between both location (edges in the graph)\n",
    "\n",
    "So, for each person we :\n",
    "\n",
    "* create a node corresponding to locations\n",
    "* ignore institutions\n",
    "* create an edge between nodes for each place mentioned\n",
    "* TODO: properly store the \"start\" and \"end\" of the link.\n",
    "* TODO: add names of the politicians in the edge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final size of the graph : 1419 nodes (places), 9105 edges (common people)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations \n",
    "\n",
    "G = nx.Graph() # main graph\n",
    "\n",
    "for bio in db.biographies.find():\n",
    "    nodes = []\n",
    "    for row in bio[\"career\"]:\n",
    "        for link in row[\"links\"]:\n",
    "            \n",
    "            if link[\"type\"] == \"location\":\n",
    "                try : \n",
    "                    # create node\n",
    "                    node = create_node(\n",
    "                        link[\"name\"], \n",
    "                        link[\"type\"], \n",
    "                        row[\"start\"], \n",
    "                        row[\"end\"], \n",
    "                        info={ \"url\" : link[\"url\"] }\n",
    "                    )\n",
    "\n",
    "                    nodes.append(node)\n",
    "\n",
    "                except UnicodeEncodeError:\n",
    "                    print \"UnicodeEncodeError\"\n",
    "        \n",
    "        \n",
    "    # create edges\n",
    "    # TODO : for now\n",
    "    for e in list(combinations(set(nodes), 2)):\n",
    "        merge_edge_data(G, e, { \"name\" : bio[\"name\"], \"mongo_id\" : bio[\"_id\"]} )\n",
    "\n",
    "print \"Final size of the graph : %s nodes (places), %s edges (common people)\"%(len(G.nodes()), len(G.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Province/county geographic hierarchy\n",
    "\n",
    "The data format relies on a specific classification of county/province/cities.\n",
    "To make the graph more explicit, we decide to aggregate by province.\n",
    "\n",
    "TODO: aggregate by county, or at the city level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "911 sub-regions parsed\n"
     ]
    }
   ],
   "source": [
    "region_pages = []\n",
    "\n",
    "for n in G.nodes(data=True):\n",
    "    if \"region\" in n[0] or \"province\" in n[0]: \n",
    "        region_pages.append((n[0], n[1][\"info\"][\"url\"]))\n",
    "\n",
    "# for stats\n",
    "unknown_slugs = []\n",
    "all_slugs = []\n",
    "\n",
    "# parse subregions index\n",
    "sub_to_regions = {}\n",
    "for r in region_pages:\n",
    "    region_slug = r[0]\n",
    "    path = \"data/regions/\"+region_slug+\".json\"\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\") as f:\n",
    "            sub_regions = json.load(f)\n",
    "        \n",
    "        for sub in sub_regions[\"children\"]:\n",
    "            slug = get_slug(sub[\"short_name\"], \"location\")\n",
    "            all_slugs.append(slug)\n",
    "            try: \n",
    "                G[slug]\n",
    "                sub_to_regions[slug] = region_slug\n",
    "            except KeyError:\n",
    "                unknown_slugs.append(slug)\n",
    "                \n",
    "print \"%s sub-regions parsed\"%len(sub_to_regions.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping relationships between regions\n",
    "\n",
    "We first map relationships between regions. \n",
    "\n",
    "For each place mentioned in the graph, we aggregate the cities and counties into the province they belong to.\n",
    "\n",
    "Then, we use the software [Topogram](http://topogram.io) to create a map of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regional graph: 33 nodes (provinces), 193 edges \n",
      "27 nodes\n",
      "193 edges\n",
      "Creating topogram 'China Political Networks'\n",
      "A topogram with the same name already exists\n",
      "nodes deleted\n",
      "edges deleted\n",
      "27 nodes created.\n",
      "193 edges created.\n",
      "done. Topogram is online at https://app.topogram.io/topograms/PYCHNaBubGHLErZXo/view\n"
     ]
    }
   ],
   "source": [
    "# create graph by region\n",
    "G_by_regions = nx.Graph()\n",
    "\n",
    "# create nodes\n",
    "\n",
    "for r in region_pages:\n",
    "    slug = r[0]\n",
    "    G_by_regions.add_node(slug)\n",
    "    \n",
    "\n",
    "for e in G.edges():\n",
    "    if e[0] in sub_to_regions.keys() and e[1] in sub_to_regions.keys():\n",
    "        source = sub_to_regions[e[0]]\n",
    "        target = sub_to_regions[e[1]]\n",
    "        \n",
    "        if G_by_regions.has_edge(source, target):\n",
    "            G_by_regions[source][target]['weight'] += 1\n",
    "        else:\n",
    "            G_by_regions.add_edge(source, target, weight=1)\n",
    "        \n",
    "\n",
    "print \"Regional graph: %s nodes (provinces), %s edges \"%(len(G_by_regions.nodes()), len(G_by_regions.edges()))\n",
    "\n",
    "\n",
    "# Create the interactive map\n",
    "nodes = parse_nodes(G_by_regions)\n",
    "edges = parse_edges(G_by_regions)\n",
    "create_topogram(\"China Political Networks\", nodes, edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping relationships from Shenzhen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "54 nodes\n",
      "53 edges\n",
      "Creating topogram 'Shenzhen Links'\n",
      "A topogram with the same name already exists\n",
      "nodes deleted\n",
      "edges deleted\n",
      "54 nodes created.\n",
      "53 edges created.\n",
      "done. Topogram is online at https://app.topogram.io/topograms/WhCjLGZFL5JMfEGZr/view\n"
     ]
    }
   ],
   "source": [
    "import html2text # used to parse text\n",
    "\n",
    "G_shenzhen = nx.Graph()\n",
    "\n",
    "\n",
    "# select only shenzhen\n",
    "for n in G.nodes():\n",
    "    if \"shenzhen\" in n: # only the city of shenzhen\n",
    "        shenzhen_id = n\n",
    "\n",
    "shenzhen_targets = G[shenzhen_id]\n",
    "\n",
    "G_shenzhen.add_node(n,G.node[shenzhen_id])\n",
    "\n",
    "print len(shenzhen_targets)\n",
    "# print shenzhen_links\n",
    "\n",
    "# store names of the guys\n",
    "\n",
    "for target in shenzhen_targets:\n",
    "    additionalInfo= \"\\n\" \n",
    "    weight = 0\n",
    "    \n",
    "    for info in shenzhen_targets[target][\"edge_types\"]:\n",
    "        weight = weight+1\n",
    "        \n",
    "        # get some precisions\n",
    "        bio = db.biographies.find_one(info[\"mongo_id\"])\n",
    "        additionalInfo += \"\"\"#### [%s](%s) \\n\n",
    "\\n\n",
    "**%s**\\n\n",
    "%s\\n\n",
    "\"\"\"%(bio[\"name\"], bio[\"url\"], bio[\"title\"], html2text.html2text(bio[\"biography\"]))\n",
    "        \n",
    "        \n",
    "    \n",
    "    e = {}\n",
    "    e[\"additionalInfo\"] = additionalInfo\n",
    "    e[\"weight\"] = weight\n",
    "    G_shenzhen.add_edge(shenzhen_id, target, **e)\n",
    "    \n",
    "    \n",
    "    # store data\n",
    "    G_shenzhen.add_node(target,G.node[target])\n",
    "\n",
    "# Create the interactive map\n",
    "nodes = parse_nodes(G_shenzhen)\n",
    "edges = parse_edges(G_shenzhen)\n",
    "create_topogram(\"Shenzhen Links\", nodes, edges)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chengdu-city-location\n",
      "67\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chengdu_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-3aaba5870b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchengdu_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mchengdu_links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchengdu_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chengdu_links' is not defined"
     ]
    }
   ],
   "source": [
    "import html2text # used to parse text\n",
    "\n",
    "G_chengdu = nx.Graph()\n",
    "\n",
    "\n",
    "# select only chengdu\n",
    "for n in G.nodes():\n",
    "    if \"chengdu\" in n: # only the city of chengdu\n",
    "        print n\n",
    "        chengdu_id = n\n",
    "\n",
    "chengdu_targets = G[chengdu_id]\n",
    "\n",
    "G_chengdu.add_node(n,G.node[chengdu_id])\n",
    "\n",
    "print len(chengdu_targets)\n",
    "\n",
    "for target in chengdu_targets:\n",
    "    additionalInfo= \"\\n\" \n",
    "    weight = 0\n",
    "    \n",
    "    for info in chengdu_targets[target][\"edge_types\"]:\n",
    "        weight = weight+1\n",
    "        \n",
    "        # get some precisions\n",
    "        bio = db.biographies.find_one(info[\"mongo_id\"])\n",
    "        additionalInfo += \"\"\"#### [%s](%s) \\n\n",
    "\\n\n",
    "**%s**\\n\n",
    "%s\\n\n",
    "\"\"\"%(bio[\"name\"], bio[\"url\"], bio[\"title\"], html2text.html2text(bio[\"biography\"]))\n",
    "        \n",
    "        \n",
    "    \n",
    "    e = {}\n",
    "    e[\"additionalInfo\"] = additionalInfo\n",
    "    e[\"weight\"] = weight\n",
    "    G_chengdu.add_edge(chengdu_id, target, **e)\n",
    "    \n",
    "    \n",
    "    # store data\n",
    "    G_chengdu.add_node(target,G.node[target])\n",
    "\n",
    "# Create the interactive map\n",
    "nodes = parse_nodes(G_chengdu)\n",
    "edges = parse_edges(G_chengdu)\n",
    "create_topogram(\"Mobility of officials around Chengdu\", nodes, edges)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
